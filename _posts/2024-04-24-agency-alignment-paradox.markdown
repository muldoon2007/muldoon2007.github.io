---
title: The AI Agency Narrowing Effect
layout: post
date: 2024-04-24 -0500
categories:
- AI,
- Alignment
---

I submit the following as the *AI Agency Alignment Narrowing Effect*:

1. It is broadly recognized that aligning AI systems to human values
   is a critical, if not *the* critical, engineering problem in safe
   AI development.
2. One of our values we seek to align to AI systems is as follows:

    > We wish to expand human agency: people's capacity to think and
    > decide for themselves, to feel the power to improve some part of
    > life, and take responsibility for their own decisions.

3. When we utilize AI for anything beyond information-searching or
   pattern-matching -- when we trust the opinions, recommendations, or
   suggestions of an AI -- we also transfer agency from the human
   realm to the machine realm. This directly contradicts the value
   stated in Point 2.

4. Therefore, AI systems should more closely resemble pure
   information-searching and pattern-matching systems as they become
   more aligned with Point 2. They should feel less like agents, and
   more like search engines.

I call it a "narrowing effect" because Point 4 reduces the
capabilities of the AI system along some dimensions: art generation,
AI therapy, group deliberations, event and travel planning. It also
precludes much of the "AI Agents" capabilities that seem to be the
next big thing from the big AI shops.
