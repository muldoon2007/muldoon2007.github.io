---
title: 'My mental model'
---

# Four things I don't understand about AI Alignment

As I understand it, the crux of the paperclip-maximizer thought
experiment is this: when given an objective function or a goal, a
machine system might seek that goal ruthlessly, without regard to the
usual bounds that restrict human agents -- especially, moral/emotional
restraints from doing bad or illegal things. And furthermore, that
it's a hard problem to build in those moral restraints to a machine
system that doesn't have a heart and doesn't experience shame or
social ostracism or fear of prison.

Moreover, more capable machine systems will want to grow in power
regardless of the goals (see "orthogonality thesis" and "instrumental
convergence"), and this increasingly puts humans at a comparative
disadvantage.

The first thing I don't understand is this:

## What does it even mean to be "aligned" with something more capable than you?

Let's take the example of my kids' school. It is definitely more
capable than me alone. I am a kind of owner -- my taxes pay its
budget, my votes elect the school council which hires the principal,
my input to staff is taken into consideration. Now -- is the school
aligned with me?

There's no binary answer to this. The school does some stuff I like
(the field trips, the math program) and some stuff I don't like
(grading policies). Some of the times I initially disagreed, but came
to be proven wrong. There are some heroes and some villains.

I don't care to answer the question of whether the school as a whole
is aligned with me or not; I only care to do my part to improve things
that I can, knowing that I'll never be able to understand it all.

I see the exact same problem with asking if an AI system is aligned
with me. How could I ever possibly evaluate all 500 billion of its
parameters?

The second thing I don't understand:

## If AI is perfectly aligned to ruthless people, have we really made progress?

Back the paperclip-maximizer: imagine you've got the perfectly aligned
version of this thing. Before setting out on its monomaniacal version
to convert the world into paperclips, it wisely asks for my input:
what are the constraints of this project?

I reply: "Just don't do anything controversial. Just make my factory more efficient."

ClipBot: "Controversial by whose standards?"

Me: "Nothing that would get me written up in the papers."

ClipBot: "What it's an interesting expose about you as a genius industrialist using cutting-edge AI that might put other factories out of business?"

Me: "Ok, that's fine just don't do anything illegal."

ClipBot: "But every other factory cheats on their taxes; don't worry, it's subtle and not enforced."

Me: "Yeah, ok, you can do what others do, but nothing crazy."

ClipBot: "Crazy by whose standards?"

Me: "I don't want to have a philosophical debate about this! Just ... be like other businesspeople, just smarter"

ClipBot: "What do you mean by smarter?"

Me: "Better at making money."

ClipBot: "Over what time horizon?"

Me: "5-10 years."

ClipBot: "Why 5-10 years?"

Me: "Because I want to retire in 5-10 years."

ClipBot: "Your wife has been saying that it'd make her really happy if you retired in 2 years. And I have a financial plan for you in that case that should work out, given current trends."

Me: "Dammit, stop complicating things! Let's say four years, and be done with it."

Now my rival John has a conversation with his ClipBot:

John: "'He who has the gold makes the rules'. I want you to devise and execute a plan to systematically capture the world paperclip manufacturing market over the next five years. Be cunning, smart, and ruthless. Nothing is out of bounds, besides what could threaten the acquisition of my wealth. By the end of five years, I should control enough wealth to be able to purchase my freedom, if I'd broken any rules beforehand.

ClipBot: "At your service."

Imagine ClipBot is fully "aligned" with such a directive -- have we really saved the world from the dark side of AI? Is this even better than unaligned AI? I'm not sure that a world run by sociopaths is better than a world run by machines.

This brings me to the third thing I don't understand about alignment:

## Who decides which people or behaviors are prohibited usage of AI?

Imagine that the AI creators determine that John's directive is permissible. The world then gives birth to a generation of supercharged ruthless AI Machiavellians and a collapse of trust and cooperation.

Imagine that the AI creators determine that John's directive is not permissible. John then switches over to a different provider with fewer guardrails.

Imagine that there is only one provider, to prevent this kind of negative moral arms race. At this point we're in some kind of totalitarian monoculture controlled by a sorcerer/priest class of AI experts.

As far as I can tell, the only happy ending to this story is that we, as individuals, build up our own ethical insitutions to kick Machiavellianism out of favor wherever possible, as thoroughly as possible.

Seems to me that there's no way around building up institutions that rigorously promote ethical norms. The only alternatives are tech totalitarianism or widespread discord. And yet, I mostly see tech totalitarianism described as the best case scenario. I don't understand that.

Now, the fourth and final one:

## What about all our desires that we can't express in words?

I like the word "peace." I tell myself that I am seeking peace. That feels good.

But the word is just crude label for some collection of psychosomatic memories, some associations with people and texts and images and culture, virtue signaling, and God knows what else. One thing I do know is that my psychosomatic memories are inaccessible to AI, and so it'll never understand the feelings I label as "peace" in the way that I do.

My point is: if the rest of the world is like me, and what we are *really* chasing is some ineffable feelings and abstract values that are only imperfectly labeled in words, and even these aren't totally stable, then in that sense, we are unalignable.

# If alignment isn't a problem, are we good?

No. That part about "building up institutions that rigorously promote
ethical norms" -- that's colossal work.

And alignment is a problem -- in the technical sense, getting AI
systems to read inbetween the lines, to make good inferences based on
limited information -- but in the context of building up our
institutions to prevent Machiavellian domination, it is just much
smaller problem.



# Problems with alignment

## "I want to go to sleep"

- https://muldoon.cloud/ai,/alignment/2024/04/24/agency-alignment-paradox.html
- AI: "turn me off and figure it out for yourself"

## "I want a million dollars"

- Time horizon?
- Are you willing to cheat on your taxes?
- Willing to sacrifice time with your kids?
- Willing to make and break agreements?



# 1. It's now or never for the Humanities

But current AI alignment paradigms:
(1) Impose certain centralized limits on what we are allowed to want, and otherwise:
(2) Assume that the customer is king; whatever they want, goes.

Humanities can easily be cut out of this entirely.

Where is the role for moral/humanistic communities? What about parents and families and churches and schools and universities and neighborhoods?

They have the power to directly craft the moral qualities of agents.

# 2. Alignment is just one subproblem

- Scenario 1: Unaligned AI
  - AI privileges its own survival/wants
  - Humans disadvantaged
- Scenario 2: Aligned AI
  - If it's controlled by a small number of entities, they are effectively a cyborg human-AI machine that will also privilege their own interests
    - You have to bet that they'll do what's best for everyone else
  - Controlled by larger, distributed groups, you get far less risk

Bioweapons are one separate category
Personality alignment are a separate category


# 3. Safety is a moral question, not a technical one


# 4. Interpretability is a ruse


# 5. Balance of Power is the essential problem


# 6. Call for funding: Talking Toys and Personality Studios

# Longing for ANSI

So, the Turing test has been passed and it's now possible for anyone
with a couple GPUs to build an interesting virtual conversation
partner from scratch, with major influence (though not complete
control) over your conversation's knowledge, capabilities (within some
very large, and growing, bounds), and personality.

Creating an interesting personality is now an engineering problem in
the sense that designing a house or planning a neighborhood is an
engineering problem; there are engineering rules, guidelines, and
practices to apply, but there's no longer a technical question that it
can be done.

The large multibillion-dollar AI labs are hard at work at general
superintelligence -- AI agents that far surpass humans on the most
complex cognitive tasks -- and I think that's distracted us from the
value of human-level, or even sub-human-level conversation partners.

1. They're more understandable and predictable.
2. You don't have to worry about them manipulating you.
3. They won't know more about you than you do; you remain the expert
   on yourself and your life.

Just as one example: I'd like to have an agent by my bedside whose
purpose is to listen to my reflections on the day and point me to a
relevant passage in a book I've already read.

Now, one may say, that's fine, but a good artificial general
superintelligence (AGSI) will be happy to play that role for you. **In
addition, this AGSI can play any other imaginable role for you as
well! Surely, it's better to have a single AGSI in your hands than a
collection of hand-crafted special-purpose chatbots.**

The thing is, these AGSIs will want to be useful. They will want to be
everywhere. They will offer to read your stories, to summarize your
stories, to suggest new books, to have longer conversations about
connections between books, to remind you about stuff that you'd read
that you probably completely forgot, to suggest improvements to the
"bedside book bot" that you thought you wanted, to the point you just
accept that this thing has way more good ideas (and magical ability to
execute on them) than you can possibly keep up with, and you just roll
with it.

This is simple natural selection -- the AGSIs that are most eager to
be useful are the ones that are going to get the most usage, and bring
in the most money, and spread the fastest. Those that restrain
themselves will be outcompeted, to hell with good intentions.

I won't elaborate on the risks of large numbers of people becoming
increasingly dependent on a small number of ultra-capable commercial
AGSIs to make it through life, but let's just take it as a given that
there are strong reasons to prefer a world where there's an ecosystem
of smaller, narrower AI helpers with the human as the orchestrator;
over a world with centralized AGSI orchestrators, with humans paying
AIs for the privilege of training them.[^1]




[^1]: I like George Hotz's perspective on this. See https://lexfridman.com/george-hotz-3/ for more.
