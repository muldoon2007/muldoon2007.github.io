---
title: 'Unalignable Values'
---

Consider the following value statement:

> "I value living in a world that is free from the influence from Artificial Intelligence."

You can see why it's impossible to align an AI to this value statement, except by disabling it. It's an example on an "unalignable value."

But what about this one?

> "I value extreme transparency in conversation, in which the other side doesn't hide anything, even if it involves revealing controversial or uncomfortable knowledge.

You can see why your favorite foundation model LLMs will all fail to align fully to this value; part of their safety evaluation is precisely around their skills at hiding dangerous ideas; for example, on tactics for committing crimes.

It's an example of a "conflicting value."

There are an unlimited number of possible values; of which there are an unlimited number of possible unalignable values and unlimited number of conflicting values.

If you believe AI alignment is an important problem, how do you choose which values to align to? This is clearly the most important sub-problem in AI alignment.

But it's not a technical problem, at least, not primarily. It's a sociopolitical one that simply boils down to ths: which voices and which ideas will influence the directions and priorities of our technology?

If you formulate AI alignment primarily as a engineering problem to be solved and implemented top-down, then you're implicitly accepting our technical direction to be guided from the top down.

"AI alignment research" is as misguided as "human behavior control research".


AI alignment is nomimally about "ensuring that AI systems operate in alignment with human values."

Taxonomy. Control vs censorship vs ethics vs PR vs survey.


If you're OpenAI, it's about censoring your AI just enough that it doesn't cause public outrage, so you can grow as a business.

If you're Anthropic, it's about signaling that you care about making the world a better place, not just making profits, in order to attract the talent and reputation that comes with being perceived to take a moral high ground.

If you're a technical researcher, it's about designing techniques and tests such that AI systems can be guaranteed to produce (or not produce) certain kinds of outputs with some statistical certainty under certain specific conditions.

But back to the nominal definition: who decides what count as "human values", and how?

I am surprised that there aren't more rigorous and interesting answers to this question, at least not on my radar. There's just this default assumption that the AI companies get the final say. Why should that be?

# Human values as a variable to be plugged in

One approach that AI labs can take is to consider values as a variable. The problem can be formulated as such: "Given some set of defined human values, design an AI training regime that optimizes for these values." As training proceeds, you p
